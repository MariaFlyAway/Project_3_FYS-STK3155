{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and embedding\n",
    "This file includes code which classifies text chunks as (Austen, Cervantes, Sturluson, God and Dostoyevsky).\n",
    "The training data is text chunks from their respective works _Pride and predjudice_, _King James Bible_ (Genesis through Deuteronomy), _Don Quixote_, _Heimskringla_ and _Crime and punishment_. We obtain the texts from the Gutenberg Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from nltk import tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\fysstk-textembedding\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\maria\\anaconda3\\envs\\fysstk-textembedding\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\maria\\.cache\\huggingface\\hub\\models--mixedbread-ai--mxbai-embed-large-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Importing the transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\" \n",
    "\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for reading in the desired parts of the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocessing(filepath, text, end=False):\n",
    "    if 'austen' in filepath:\n",
    "        start = text.find(\"Chapter I.]\")\n",
    "\n",
    "    elif 'dostoyevsky' in filepath:\n",
    "        start = text.find(\"CHAPTER I\")\n",
    "\n",
    "    elif 'god' in filepath:\n",
    "        start = text.find(\"1:1\")\n",
    "        end = text.find(\"in the sight of all Israel.\") # Only old testament\n",
    "\n",
    "    elif 'kafka' in filepath:\n",
    "        start = text.find(\"Chapter One\")\n",
    "\n",
    "    elif 'shelley' in filepath:\n",
    "        start = text.find(\"_To\")\n",
    "\n",
    "    elif 'tolstoy' in filepath:\n",
    "        start = text.find(\"Chapter 1\")\n",
    "\n",
    "    elif 'sturluson' in filepath:\n",
    "        start = text.find(\"PREFACE OF SNORRE STURLASON.\")\n",
    "        end = text.find(\"SAGA OF HARALD HARDRADE.\") # Only Heimskringla\n",
    "        \n",
    "    elif 'cervantes' in filepath:\n",
    "        start = text.find(\"Idle reader:\")\n",
    "\n",
    "    elif 'brother_karamazov' in filepath:\n",
    "        start = text.find(\"Fyodor Pavlovitch Karamazov\")\n",
    "\n",
    "    elif 'sense_and_sensibility' in filepath:\n",
    "        start = text.find(\"CHAPTER I.\")\n",
    "\n",
    "    elif 'wells' in filepath:\n",
    "        start = text.find(\"Introduction\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"This book is not in our library!\")\n",
    "    \n",
    "    if not end:\n",
    "        end = text.find(\"*** END\")\n",
    "    \n",
    "    return text[start:end].split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for splitting the sentences using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(filepath, Feedback=True):\n",
    "    with open(filepath, encoding='utf-8') as infile:\n",
    "        words = preprocessing(filepath, infile.read())\n",
    "        words = ' '.join(words)\n",
    "\n",
    "        sentences = tokenize.sent_tokenize(words)\n",
    "\n",
    "    if Feedback:\n",
    "        print(f\"Length: {len(sentences):,} sentences.\")\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for embedding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embeddings.append(model.encode(chunk))\n",
    "    return np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding all the sentences, the sentences are added to a list for use later\n",
    "\n",
    "Do note that this was not the ideal way of doing this, but the sentences were embedded first, so it was done this way to avoid having to embed the sentences twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Texts\\austen\n",
      "Length: 4,657 sentences.\n",
      "..\\Texts\\dostoyevsky\n",
      "Length: 11,906 sentences.\n",
      "..\\Texts\\god\n",
      "Length: 4,962 sentences.\n",
      "..\\Texts\\cervantes\n",
      "Length: 5,883 sentences.\n",
      "..\\Texts\\sturluson\n",
      "Length: 8,781 sentences.\n"
     ]
    }
   ],
   "source": [
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "filenames = ['austen', 'dostoyevsky', 'god', 'cervantes', 'sturluson']\n",
    "filepaths = [os.path.join(folder, subfolder, filename) for filename in filenames]\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "# Uncomment to embed\n",
    "for filepath in filepaths:\n",
    "    print(filepath)\n",
    "    sentences = split_sentences(filepath+'.txt') # remove max_len to run on entire text\n",
    "    all_sentences.append(sentences)\n",
    "    embeddings = embed(sentences)\n",
    "    np.save(filepath+'_sentences', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick 2500 sentences from each author and filter out the sentences that are under 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2500\n",
    "sent_reduced = []\n",
    "embeddings_red = []\n",
    "length_threshold = 100\n",
    "filepath = '../Texts/'\n",
    "files = ['austen_sentences.npy', 'dostoyevsky_sentences.npy', 'god_sentences.npy', 'cervantes_sentences.npy', 'sturluson_sentences.npy']\n",
    "\n",
    "for idx, sent in enumerate(all_sentences):\n",
    "    sentences_array = np.array(sent)\n",
    "    embedding = np.load(filepath+files[idx])\n",
    "\n",
    "    # Create a vectorized function to check the length of each sentence\n",
    "    length_check = np.vectorize(lambda s: len(s) >= length_threshold)\n",
    "\n",
    "    # creating a mask and applying it to the sentences and embeddings\n",
    "    mask = length_check(sentences_array)\n",
    "    filtered_sentences = sentences_array[mask]\n",
    "    embedding = embedding[mask]\n",
    "\n",
    "    # picking out 2500 random sentences\n",
    "    indices = np.random.choice(np.arange(0, len(filtered_sentences)), size=n, replace=False)\n",
    "\n",
    "    filtered_sentences = filtered_sentences[indices]\n",
    "    embedding = embedding[indices]\n",
    "\n",
    "    sent_reduced.append(filtered_sentences)\n",
    "    embeddings_red.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(embeddings_red)\n",
    "y = np.repeat(np.arange(5), 2500) \n",
    "\n",
    "np.save('text_data_sentences', X)\n",
    "np.save('labels_sentences', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe with embeddings and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sent_reduced).ravel()\n",
    "embedding = np.array(embeddings_red).reshape(-1, 1024)\n",
    "\n",
    "df = pd.DataFrame(embedding, columns=[f\"component {i}\" for i in range(embedding.shape[1])])\n",
    "df.insert(0, \"sentence\", sentences)\n",
    "\n",
    "df.to_csv('../Classification/sentences_embeddings', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk-textembedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
