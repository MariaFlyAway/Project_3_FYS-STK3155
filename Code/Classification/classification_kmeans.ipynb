{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\FYS-STK3155\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Reduction UMAP 150 Chunk Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Data/text_data150.npy')\n",
    "y = np.load('Data/labels150.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 2\n",
      "Dimension: 4\n",
      "Dimension: 8\n",
      "Dimension: 16\n",
      "Dimension: 32\n",
      "Dimension: 64\n",
      "Dimension: 128\n",
      "Dimension: 256\n",
      "Dimension: 512\n",
      "Dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "sizes = np.arange(1, 11)\n",
    "dimensions = 2 ** np.arange(1, 11)\n",
    "rand_scores_hdbscan = np.zeros((len(dimensions), len(sizes)))\n",
    "rand_scores_kmeans = np.zeros((len(dimensions), len(sizes)))\n",
    "all_labels_hdbscan = np.zeros((len(dimensions), len(sizes)), dtype=object)\n",
    "all_labels_kmeans = np.zeros((len(dimensions), len(sizes)), dtype=object)\n",
    "\n",
    "for idy, dim in enumerate(dimensions):\n",
    "    print(f'Dimension: {dim}')\n",
    "    for idx, size in enumerate(sizes):\n",
    "        if size == 10:\n",
    "            X_val, y_val = X_train, y_train\n",
    "        else:\n",
    "            X_t2, X_val, y_t2, y_val = train_test_split(X_train, y_train, test_size=size/10, stratify=y_train, random_state=3)\n",
    "\n",
    "        if dim == 1024:\n",
    "            embedding_train = X_train\n",
    "        else:\n",
    "            mapper = umap.UMAP(n_neighbors=30,\n",
    "                                min_dist=0.0,\n",
    "                                init='random',\n",
    "                                metric='euclidean',\n",
    "                                n_components=dim).fit(X_val, y=y_val)\n",
    "            embedding_train = mapper.transform(X_train)\n",
    "\n",
    "        labels_hdb = hdbscan.HDBSCAN(\n",
    "            min_samples=10,\n",
    "            min_cluster_size=500,\n",
    "        ).fit_predict(embedding_train)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=5, random_state=3)\n",
    "        kmeans.fit(embedding_train)\n",
    "        labels_kmeans = kmeans.labels_\n",
    "\n",
    "        rand_scores_hdbscan[idy,idx] = adjusted_rand_score(y_train, labels_hdb)\n",
    "        rand_scores_kmeans[idy, idx] = adjusted_rand_score(y_train, labels_kmeans)\n",
    "\n",
    "        all_labels_hdbscan[idy, idx] = labels_hdb\n",
    "        all_labels_kmeans[idy, idx] = labels_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Results/labels_hdbscan_150.npy', all_labels_hdbscan)\n",
    "np.save('Results/labels_kmeans_150.npy', all_labels_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0],\n",
       "       [   9,    0,    0,  644,    0,    0],\n",
       "       [  29,    0,    0,    2,   20, 1029],\n",
       "       [   1,  866,    0,    0,    0,    0],\n",
       "       [  34,    1,    1,    0, 1023,    2],\n",
       "       [   9,    1, 1061,    0,    0,    2]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, all_labels_hdbscan[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Reduction PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 2\n",
      "Dimension: 4\n",
      "Dimension: 8\n",
      "Dimension: 16\n",
      "Dimension: 32\n",
      "Dimension: 64\n",
      "Dimension: 128\n",
      "Dimension: 256\n",
      "Dimension: 512\n",
      "Dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "dimensions = 2 ** np.arange(1, 11)\n",
    "rand_scores_hdbscan = np.zeros(len(dimensions))\n",
    "rand_scores_kmeans = np.zeros(len(dimensions))\n",
    "all_labels_hdbscan = np.zeros(len(dimensions), dtype=object)\n",
    "all_labels_kmeans = np.zeros(len(dimensions), dtype=object)\n",
    "\n",
    "for idx, dim in enumerate(dimensions):\n",
    "    print(f'Dimension: {dim}')\n",
    "    X_train_red = PCA(n_components=dim).fit_transform(X_train)\n",
    "\n",
    "    labels_hdb = hdbscan.HDBSCAN(\n",
    "        min_samples=10,\n",
    "        min_cluster_size=500,\n",
    "    ).fit_predict(X_train_red)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5, random_state=3)\n",
    "    kmeans.fit(X_train_red)\n",
    "    labels_kmeans = kmeans.labels_\n",
    "\n",
    "    rand_scores_hdbscan[idx] = adjusted_rand_score(y_train, labels_hdb)\n",
    "    rand_scores_kmeans[idx] = adjusted_rand_score(y_train, labels_kmeans)\n",
    "\n",
    "    all_labels_hdbscan[idx] = labels_hdb\n",
    "    all_labels_kmeans[idx] = labels_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Results/labels_hdbscan_PCA150.npy', all_labels_hdbscan)\n",
    "np.save('Results/labels_kmeans_PCA150.npy', all_labels_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component reduction with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Data/text_data_sentences.npy')\n",
    "y = np.load('Data/labels_sentences.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 2\n",
      "Dimension: 4\n",
      "Dimension: 8\n",
      "Dimension: 16\n",
      "Dimension: 32\n",
      "Dimension: 64\n",
      "Dimension: 128\n",
      "Dimension: 256\n",
      "Dimension: 512\n",
      "Dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "sizes = np.arange(1, 11)\n",
    "dimensions = 2 ** np.arange(1, 11)\n",
    "rand_scores_hdbscan = np.zeros((len(dimensions), len(sizes)))\n",
    "rand_scores_kmeans = np.zeros((len(dimensions), len(sizes)))\n",
    "all_labels_hdbscan = np.zeros((len(dimensions), len(sizes)), dtype=object)\n",
    "all_labels_kmeans = np.zeros((len(dimensions), len(sizes)), dtype=object)\n",
    "\n",
    "for idy, dim in enumerate(dimensions):\n",
    "    print(f'Dimension: {dim}')\n",
    "    for idx, size in enumerate(sizes):\n",
    "        if size == 10:\n",
    "            X_val, y_val = X_train, y_train\n",
    "        else:\n",
    "            X_t2, X_val, y_t2, y_val = train_test_split(X_train, y_train, test_size=size/10, stratify=y_train, random_state=3)\n",
    "\n",
    "        if dim == 1024:\n",
    "            embedding_train = X_train\n",
    "        else:\n",
    "            mapper = umap.UMAP(n_neighbors=30,\n",
    "                                min_dist=0.0,\n",
    "                                init='random',\n",
    "                                metric='euclidean',\n",
    "                                n_components=dim).fit(X_val, y=y_val)\n",
    "            embedding_train = mapper.transform(X_train)\n",
    "\n",
    "        labels_hdb = hdbscan.HDBSCAN(\n",
    "            min_samples=10,\n",
    "            min_cluster_size=500,\n",
    "        ).fit_predict(embedding_train)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=5, random_state=3)\n",
    "        kmeans.fit(embedding_train)\n",
    "        labels_kmeans = kmeans.labels_\n",
    "\n",
    "        rand_scores_hdbscan[idy,idx] = adjusted_rand_score(y_train, labels_hdb)\n",
    "        rand_scores_kmeans[idy, idx] = adjusted_rand_score(y_train, labels_kmeans)\n",
    "\n",
    "        all_labels_hdbscan[idy, idx] = labels_hdb\n",
    "        all_labels_kmeans[idy, idx] = labels_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Results/labels_hdbscan_sentences.npy', all_labels_hdbscan)\n",
    "np.save('Results/labels_kmeans_sentences.npy', all_labels_kmeans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYS-STK3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
