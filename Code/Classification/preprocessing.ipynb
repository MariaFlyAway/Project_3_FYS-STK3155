{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and embedding\n",
    "This file includes code which classifies text chunks as (Austen, Cervantes, Sturluson, God and Dostoevsky).\n",
    "The training data is text chunks from their respective works _Pride and predjudice_, _King James Bible_ (Genesis through Deuteronomy), _Don Quixote_ (Volume I), _Heimskringla_ and _Crime and punishment_. We obtain the texts from the Gutenberg Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\fysstk-textembedding\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Importing the transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\" \n",
    "\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for reading in the desired parts of the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocessing(filepath, text, end=False):\n",
    "    if 'austen' in filepath:\n",
    "        start = text.find(\"Chapter I.]\")\n",
    "\n",
    "    elif 'dostoyevsky' in filepath:\n",
    "        start = text.find(\"CHAPTER I\")\n",
    "\n",
    "    elif 'god' in filepath:\n",
    "        start = text.find(\"1:1\")\n",
    "        end = text.find(\"in the sight of all Israel.\") # Only Genesis thorugh Deuteronomy\n",
    "\n",
    "    elif 'kafka' in filepath:\n",
    "        start = text.find(\"Chapter One\")\n",
    "\n",
    "    elif 'shelley' in filepath:\n",
    "        start = text.find(\"_To\")\n",
    "\n",
    "    elif 'tolstoy' in filepath:\n",
    "        start = text.find(\"Chapter 1\")\n",
    "\n",
    "    elif 'sturluson' in filepath:\n",
    "        start = text.find(\"PREFACE OF SNORRE STURLASON.\")\n",
    "        end = text.find(\"SAGA OF HARALD HARDRADE.\") # Only Heimskringla\n",
    "        \n",
    "    elif 'cervantes' in filepath:\n",
    "        start = text.find(\"Idle reader:\")\n",
    "        end = text.find(\"Forse altro cantera con miglior plettro.\") # Only Volume I\n",
    "\n",
    "    elif 'brother_karamazov' in filepath:\n",
    "        start = text.find(\"Fyodor Pavlovitch Karamazov\")\n",
    "\n",
    "    elif 'sense_and_sensibility' in filepath:\n",
    "        start = text.find(\"CHAPTER I.\")\n",
    "\n",
    "    elif 'wells' in filepath:\n",
    "        start = text.find(\"Introduction\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"This book is not in our library!\")\n",
    "    \n",
    "    if not end:\n",
    "        end = text.find(\"*** END\")\n",
    "   \n",
    "    return text[start:end].split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads in the contents of the books and chunks the data into the desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(filepath, chunksize=50, max_len=0, Feedback=True):\n",
    "    \"\"\"\n",
    "    Reads text into a list of strings with the specified number of words (discards final chunk to ensure similar length).\n",
    "    \"\"\"\n",
    "    with open(filepath, encoding='utf-8') as infile:\n",
    "        words = preprocessing( filepath, infile.read() ) # list of words, preprocessed\n",
    "        if max_len:\n",
    "            assert max_len > chunksize\n",
    "            words = words[:max_len]\n",
    "\n",
    "        length_words = len(words)\n",
    "        n_chunks = int(length_words / chunksize)\n",
    "        chunks = np.empty(n_chunks, dtype=object)\n",
    "        for i, start in enumerate(range(0, length_words, chunksize)):\n",
    "            if start+chunksize < length_words: # we discard the final chunk if it is shorter than 50 words\n",
    "                chunks[i] = ' '.join(words[start:start+chunksize])\n",
    "\n",
    "        if Feedback:\n",
    "            print(f\"Length: {length_words:,} words, on {n_chunks:,} chunks of length {chunksize}.\")\n",
    "        \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for embedding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embeddings.append( model.encode(chunk) )\n",
    "    return np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding all the data and creating a dataframe with the sentences and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Texts\\wells\n",
      "Length: 32,442 words, on 216 chunks of length 150.\n"
     ]
    }
   ],
   "source": [
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "filenames = ['austen', 'dostoyevsky', 'god', 'cervantes', 'sturluson']\n",
    "filepaths = [os.path.join(folder, subfolder, filename) for filename in filenames]\n",
    "\n",
    "# Uncomment to embed\n",
    "df = pd.DataFrame({\n",
    "    'author': pd.Series(dtype='int'),\n",
    "    'text': pd.Series(dtype='str'), \n",
    "    'embedding': pd.Series(dtype='object')\n",
    "})\n",
    "\n",
    "for author_idx, filepath in enumerate(filepaths):\n",
    "    chunks = read_chunks(filepath+'.txt', chunksize=150, max_len=301) # remove max_len to run on entire text\n",
    "    embeddings = embed(chunks)\n",
    "    author = np.ones_like(chunks)*author_idx\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'author': author, 'text': chunks, 'embedding': embeddings})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking the embeddings and labels and savinng them as `npy`-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class encoding (0 through 4, alphabetically)\n",
    "data = [np.load(filepath+'.npy') for filepath in filepaths]\n",
    "lengths = [len(author) for author in data]\n",
    "\n",
    "X = np.vstack( data )\n",
    "y = np.repeat(np.arange(5), lengths) # improved target vector\n",
    "\n",
    "np.save('text_data', X)\n",
    "np.save('labels', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk-textembedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
