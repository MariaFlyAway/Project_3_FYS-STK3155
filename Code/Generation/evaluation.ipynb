{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries and loading in the sentence transformer model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# Importing the transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\" \n",
    "\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for picking out the parts we want from each novel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocessing(filepath, text, end=False):\n",
    "    if 'austen' in filepath:\n",
    "        start = text.find(\"Chapter I.]\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'dostoyevsky' in filepath:\n",
    "        start = text.find(\"CHAPTER I\")\n",
    "\n",
    "    elif 'god' in filepath:\n",
    "        start = text.find(\"1:1\")\n",
    "        end = text.find(\"in the sight of all Israel.\") # Only old testament\n",
    "\n",
    "    elif 'kafka' in filepath:\n",
    "        start = text.find(\"Chapter One\")\n",
    "\n",
    "    elif 'shelley' in filepath:\n",
    "        start = text.find(\"_To\")\n",
    "\n",
    "    elif 'tolstoy' in filepath:\n",
    "        start = text.find(\"Chapter 1\")\n",
    "\n",
    "    elif 'sturluson' in filepath:\n",
    "        start = text.find(\"PREFACE OF SNORRE STURLASON.\")\n",
    "        end = text.find(\"SAGA OF HARALD HARDRADE.\") # Only Heimskringla\n",
    "        \n",
    "    elif 'cervantes' in filepath:\n",
    "        start = text.find(\"Idle reader:\")\n",
    "        end = text.find(\"Forse altro cantera con miglior plettro.\") # Only Volume I\n",
    "\n",
    "    elif 'brother_karamazov' in filepath:\n",
    "        start = text.find(\"Fyodor Pavlovitch Karamazov\")\n",
    "\n",
    "    elif 'sense_and_sensibility' in filepath:\n",
    "        start = text.find(\"CHAPTER I.\")\n",
    "\n",
    "    elif 'wells' in filepath:\n",
    "        start = text.find(\"Introduction\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"This book is not in our library!\")\n",
    "    \n",
    "    if not end:\n",
    "        end = text.find(\"*** END\")\n",
    "    \n",
    "    return text[start:end]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the generator models and retrieving the file with the 100 most common English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the generators\n",
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "subfolder_generator = \"Models_and_Data\"\n",
    "filenames = ['austen', 'dostoyevsky', 'god', 'cervantes', 'sturluson']\n",
    "paths = [os.path.join(subfolder_generator, author + '_generator.pt') for author in filenames]\n",
    "generators = [torch.load(path) for path in paths]\n",
    "\n",
    "# retrieving the 100 most used english words\n",
    "with open(\"../Generation/Models_and_Data/common-words.txt\", encoding='utf-8') as infile:\n",
    "    words = infile.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for generating new text using the generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [os.path.join(folder, subfolder, filename)+'.txt' for filename in filenames]\n",
    "\n",
    "char2int = []\n",
    "char_array = []\n",
    "for i, filepath in  enumerate(filepaths):\n",
    "    with open(filepath, encoding='utf-8') as infile:\n",
    "        text = preprocessing( filepath, infile.read() ) # list of words, preprocessed\n",
    "\n",
    "        char_set = set(text)\n",
    "        chars_sorted = sorted(char_set)\n",
    "        char2int.append({ch:j for j,ch in enumerate(chars_sorted)})\n",
    "        char_array.append(np.array(chars_sorted))\n",
    "\n",
    "\n",
    "def sample(model, starting_str, author,\n",
    "           len_generated_text=500,\n",
    "           scale_factor=2.0):\n",
    "\n",
    "    encoded_input = torch.tensor([char2int[author][s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    hidden = hidden.to('cpu')\n",
    "    cell = cell.to('cpu')\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n",
    "    \n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell) \n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[author][last_char])\n",
    "        \n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating 100 chunks for each author, a sentence (90 characters) and a 150 word chunk \n",
    "torch.manual_seed(1)\n",
    "\n",
    "sentences_embedded = np.empty((5,100), object)\n",
    "chunks_embedded = np.empty((5,100), object)\n",
    "for author, generator in enumerate(generators):\n",
    "    generator.to('cpu')\n",
    "    for line, word in enumerate(words):\n",
    "        # sentence\n",
    "        sentences_embedded[author][line] = model.encode(sample(generator, word, author, len_generated_text=90, scale_factor=0.25))\n",
    "        # 150-words\n",
    "        new_words = sample(generator, word, author, len_generated_text=1000, scale_factor=0.25).split() # generating more than 150 words\n",
    "        chunk = ' '.join(new_words[1:151])\n",
    "        chunks_embedded[author][line] = model.encode(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Models_and_Data/embedded_sentences_generated_text_scale_025.npy', sentences_embedded)\n",
    "np.save('Models_and_Data/embedded_chunks_generated_text_scale_025.npy', chunks_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the generated text using the trained neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from Classification.textdataset_classification import TextDataset\n",
    "from Classification.neural_net import NeuralNet\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode \n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():           # gradient computation excluded, unnecessary\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "\n",
    "            all_preds.extend(pred.argmax(1).tolist())\n",
    "            all_labels.extend(y.tolist())\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return all_preds, all_labels, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Classification/Data\\sentences_model.pt\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.560038 \n",
      "\n",
      "Confusion Matrix:\n",
      "[[10 68  6  4 12]\n",
      " [ 5 76  4  5 10]\n",
      " [ 1 37 45  5 12]\n",
      " [ 5 52 17 12 14]\n",
      " [ 2 49 21  3 25]]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 1.493386 \n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 28  5 32 35]\n",
      " [ 0 47  8 13 32]\n",
      " [ 0  5 80  4 11]\n",
      " [ 0  7  5 26 62]\n",
      " [ 0  7 45  0 48]]\n"
     ]
    }
   ],
   "source": [
    "# Classifying (sentences)\n",
    "folder = \"../Classification/Data\"\n",
    "sentence_name = \"sentences_model.pt\"\n",
    "chunk_name = \"150_chunksize_model.pt\"\n",
    "\n",
    "sentences_embedded = np.load('../Generation/Models_and_Data/embedded_sentences_generated_text_scale_025.npy', allow_pickle=True)\n",
    "sentences_embedded = np.array([sentence for sentence in sentences_embedded.flat])\n",
    "\n",
    "X1 = torch.tensor(sentences_embedded)\n",
    "y1 = torch.from_numpy(np.repeat(np.arange(5), 100)).long()\n",
    "dataset_sentence = TextDataset(X1, y1)\n",
    "\n",
    "# import best classification model - sentences\n",
    "dl = DataLoader(dataset_sentence)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(os.path.join(folder, sentence_name))\n",
    "sentence_model = torch.load(os.path.join(folder, sentence_name))\n",
    "predictions, labels, correct = test_loop(dl, sentence_model, loss_fn)\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# Classifying (150 chunks)\n",
    "chunks_embedded = np.load('../Generation/Models_and_Data/embedded_chunks_generated_text_scale_025.npy', allow_pickle=True)\n",
    "chunks_embedded = np.array([chunk for chunk in chunks_embedded.flat])\n",
    "\n",
    "X1 = torch.tensor(chunks_embedded)\n",
    "y1 = torch.from_numpy(np.repeat(np.arange(5), 100)).long()\n",
    "dataset_chunks = TextDataset(X1, y1)\n",
    "\n",
    "# import best classification model - sentences\n",
    "dl = DataLoader(dataset_chunks)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "sentence_model = torch.load(os.path.join(folder, sentence_name))\n",
    "predictions, labels, correct = test_loop(dl, sentence_model, loss_fn)\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Accuracy|Chunk|Sentences|\n",
    "|---|---|---|\n",
    "|$\\alpha$=2|0.992|0.804|\n",
    "|$\\alpha$=1|0.992|0.756|\n",
    "|$\\alpha$=0.5|0.946|0.61|\n",
    "|$\\alpha$=0.25|0.402|0.336|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk-textembedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
