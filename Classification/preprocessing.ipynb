{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and embedding\n",
    "This file includes code which classifies text chunks as (Austen (0), Cervantes (1), Dostoyevsky (2), God (3) and Sturluson (4)).\n",
    "The training data is text chunks from their respective works 'Pride and predjudice', 'Don Quixote', 'Crime and punishment', ' The King James Bible' and 'Heimskringla'. We obtain the texts from the Gutenberg Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonas\\anaconda3\\envs\\fysstk3155\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Importing the transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\" \n",
    "\"\"\"\n",
    "Change the default model here. To save the model locally, replace the path with your path and run this module.\n",
    "At the end of your path, add the name of the folder you want create for the model, eg. \".../local_model\"\n",
    "When you have downloaded the model, uncomment model_name = path to use the stored model.\n",
    "\"\"\"\n",
    "# path = \"C:/Users/jonas/OneDrive/Dokumenter/Python Scripts/embed/local_model_sentence_transformers\" # (example path / for my convenience)\n",
    "# model_name = path # Uncomment this line once you have downloaded the model.\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filepath, text, end=False):\n",
    "    if 'austen' in filepath:\n",
    "        start = text.find(\"Chapter I.]\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'dostoyevsky' in filepath:\n",
    "        start = text.find(\"CHAPTER I\")\n",
    "\n",
    "    elif 'god' in filepath:\n",
    "        start = text.find(\"1:1\")\n",
    "        end = text.find(\"in the sight of all Israel.\") # Only Genesis thorough Deuteronomy\n",
    "\n",
    "    elif 'kafka' in filepath:\n",
    "        start = text.find(\"Chapter One\")\n",
    "\n",
    "    elif 'shelley' in filepath:\n",
    "        start = text.find(\"_To\")\n",
    "\n",
    "    elif 'tolstoy' in filepath:\n",
    "        start = text.find(\"Chapter 1\")\n",
    "\n",
    "    elif 'sturluson' in filepath:\n",
    "        start = text.find(\"PREFACE OF SNORRE STURLASON.\")\n",
    "        end = text.find(\"SAGA OF HARALD HARDRADE.\") # Only first half\n",
    "        \n",
    "    elif 'cervantes' in filepath:\n",
    "        start = text.find(\"Idle reader:\")\n",
    "        end = text.find(\"Forse altro cantera con miglior plettro.\") # Only Volume I\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"This book is not in our library!\")\n",
    "    \n",
    "    if not end:\n",
    "        end = text.find(\"*** END\")\n",
    "    \n",
    "    return text[start:end].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(filepath, chunksize=50, max_len=0, Feedback=True):\n",
    "    \"\"\"Reads text into a list of strings with the specified number of words (discards final chunk to ensure similar length).\"\"\"\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as infile:\n",
    "        words = preprocessing( filepath, infile.read() ) # list of words, preprocessed\n",
    "        if max_len:\n",
    "            assert max_len > chunksize\n",
    "            words = words[:max_len]\n",
    "\n",
    "        length_words = len(words)\n",
    "        n_chunks = int(length_words / chunksize)\n",
    "        chunks = np.empty(n_chunks, dtype=object)\n",
    "\n",
    "        for i, start in enumerate(range(0, length_words, chunksize)):\n",
    "            if start+chunksize < length_words: # we discard the final chunk if it is shorter than 50 words\n",
    "                chunks[i] = ' '.join(words[start:start+chunksize])\n",
    "\n",
    "        if Feedback:\n",
    "            print(f\"Length: {length_words:,} words, on {n_chunks:,} chunks of length {chunksize}.\")\n",
    "        \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embeddings.append( model.encode(chunk) )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 301 words, on 2 chunks of length 150.\n",
      "Length: 301 words, on 2 chunks of length 150.\n",
      "Length: 301 words, on 2 chunks of length 150.\n",
      "Length: 301 words, on 2 chunks of length 150.\n",
      "Length: 301 words, on 2 chunks of length 150.\n"
     ]
    }
   ],
   "source": [
    "# --- Main\n",
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "filenames = ['austen', 'cervantes', 'dostoyevsky', 'god', 'sturluson']\n",
    "filepaths = [os.path.join(folder, subfolder, filename) for filename in filenames]\n",
    "\n",
    "# Uncomment to embed\n",
    "df = pd.DataFrame({\n",
    "    'author': pd.Series(dtype='int'),\n",
    "    'text': pd.Series(dtype='str'), \n",
    "    'embedding': pd.Series(dtype='object')\n",
    "})\n",
    "\n",
    "for author_idx, filepath in enumerate(filepaths):\n",
    "    chunks = read_chunks(filepath+'.txt', chunksize=150, max_len=301) # remove max_len to run on entire text\n",
    "    embeddings = embed(chunks)\n",
    "    author = np.ones_like(chunks)*author_idx\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'author': author, 'text': chunks, 'embedding': embeddings})], ignore_index=True)\n",
    "\n",
    "# and then save this df..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 122,411 words, on 816 chunks of length 150.\n",
      "Length: 198,975 words, on 1,326 chunks of length 150.\n",
      "Length: 202,706 words, on 1,351 chunks of length 150.\n",
      "Length: 162,607 words, on 1,084 chunks of length 150.\n",
      "Length: 201,265 words, on 1,341 chunks of length 150.\n"
     ]
    }
   ],
   "source": [
    "# Retrieving embeddings and adding to df\n",
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "filenames = ['austen', 'cervantes', 'dostoyevsky', 'god', 'sturluson']\n",
    "filepaths = [os.path.join(folder, subfolder, filename) for filename in filenames]\n",
    "\n",
    "chunks = []\n",
    "for filepath in filepaths:\n",
    "    chunks += list(read_chunks(filepath+'.txt', chunksize=150))\n",
    "\n",
    "# Class encoding (0 through 4, alphabetically)\n",
    "data = [np.load(filepath+'.npy') for filepath in filepaths]\n",
    "lengths = [len(author) for author in data]\n",
    "X = np.vstack( data )\n",
    "y = np.repeat(np.arange(5), lengths)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'author': y,\n",
    "    'text': chunks, \n",
    "    'embedding': list(X)\n",
    "})\n",
    "\n",
    "df.to_csv(\"author_text_embedding.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
