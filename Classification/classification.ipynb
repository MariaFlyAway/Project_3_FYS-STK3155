{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "This file includes code which classifies text chunks as (Austen, Shelly, Kafka, Tolstoy or Dostoyevsky).\n",
    "The training data is text chunks from their respective works 'Pride and predjudice', 'Frankenstein', 'The trial', 'Anna Karenina' and 'Crime and punishment'. We obtain the texts from the Gutenberg Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from textdataset import TextDataset\n",
    "from neural_net import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\" \n",
    "\"\"\"\n",
    "Change the default model here. To save the model locally, replace the path with your path and run this module.\n",
    "At the end of your path, add the name of the folder you want create for the model, eg. \".../local_model\"\n",
    "When you have downloaded the model, uncomment model_name = path to use the stored model.\n",
    "\"\"\"\n",
    "# path = \"C:/Users/jonas/OneDrive/Dokumenter/Python Scripts/embed/local_model_sentence_transformers\" # (example path / for my convenience)\n",
    "# model_name = path # Uncomment this line once you have downloaded the model.\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocessing(filepath, text):\n",
    "    if 'austen' in filepath:\n",
    "        start = text.find(\"Chapter I.]\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'dostoyevsky' in filepath:\n",
    "        start = text.find(\"CHAPTER I\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'kafka' in filepath:\n",
    "        start = text.find(\"Chapter One\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'shelley' in filepath:\n",
    "        start = text.find(\"_To\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'tolstoy' in filepath:\n",
    "        start = text.find(\"Chapter 1\")\n",
    "        # potential other preprocessing\n",
    "    else:\n",
    "        raise Exception(\"This book is not in our library!\")\n",
    "    \n",
    "    end = text.find(\"*** END\")\n",
    "    return text[start:end].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(filepath, chunksize=50, max_len=0, Feedback=True):\n",
    "    \"\"\"\n",
    "    Reads text into a list of strings with the specified number of words (discards final chunk to ensure similar length).\n",
    "    \"\"\"\n",
    "    with open(filepath, encoding='utf-8') as infile:\n",
    "        words = preprocessing( filepath, infile.read() ) # list of words, preprocessed\n",
    "        if max_len:\n",
    "            assert max_len > chunksize\n",
    "            words = words[:max_len]\n",
    "\n",
    "        length_words = len(words)\n",
    "        n_chunks = int(length_words / chunksize)\n",
    "        chunks = np.empty(n_chunks, dtype=object)\n",
    "        for i, start in enumerate(range(0, length_words, chunksize)):\n",
    "            if start+chunksize < length_words: # we discard the final chunk if it is shorter than 50 words\n",
    "                chunks[i] = ' '.join(words[start:start+chunksize])\n",
    "\n",
    "        if Feedback:\n",
    "            print(f\"Length: {length_words:,} words, on {n_chunks:,} chunks of length {chunksize}.\")\n",
    "        \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embeddings.append( model.encode(chunk) )\n",
    "    return np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change max_len and run on clusters - this might take days on the full dataset (but then will never have to be done again:)\n",
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "filenames = ['austen', 'dostoyevsky', 'kafka', 'shelley', 'tolstoy']\n",
    "filepaths = [os.path.join(folder, subfolder, filename) for filename in filenames]\n",
    "\n",
    "# for filepath in filepaths:\n",
    "#     print(filepath)\n",
    "#     chunks = read_chunks(filepath+'.txt') # remove max_len to run on entire text\n",
    "#     embeddings = embed(chunks)\n",
    "#     np.save(filepath, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [4]\n",
      " [4]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "# Class encoding (0 through 4, alphabetically)\n",
    "data = [np.load(filepath+'.npy') for filepath in filepaths]\n",
    "lengths = [len(author) for author in data]\n",
    "\n",
    "X = np.vstack( data )\n",
    "y = np.vstack( [[[i]]*length for i, length in enumerate(lengths)] ) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 122,411 words, on 2,448 chunks of length 50.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chapter I.] It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_chunks('../Texts/austen.txt')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best way of doing it, but a way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 122,411 words, on 2,448 chunks of length 50.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedding_0</th>\n",
       "      <th>Embedding_1</th>\n",
       "      <th>Embedding_2</th>\n",
       "      <th>Embedding_3</th>\n",
       "      <th>Embedding_4</th>\n",
       "      <th>Embedding_5</th>\n",
       "      <th>Embedding_6</th>\n",
       "      <th>Embedding_7</th>\n",
       "      <th>Embedding_8</th>\n",
       "      <th>...</th>\n",
       "      <th>Embedding_1015</th>\n",
       "      <th>Embedding_1016</th>\n",
       "      <th>Embedding_1017</th>\n",
       "      <th>Embedding_1018</th>\n",
       "      <th>Embedding_1019</th>\n",
       "      <th>Embedding_1020</th>\n",
       "      <th>Embedding_1021</th>\n",
       "      <th>Embedding_1022</th>\n",
       "      <th>Embedding_1023</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chapter I.] It is a truth universally acknowle...</td>\n",
       "      <td>0.284331</td>\n",
       "      <td>0.198523</td>\n",
       "      <td>0.321001</td>\n",
       "      <td>-0.224241</td>\n",
       "      <td>-0.742237</td>\n",
       "      <td>-0.527830</td>\n",
       "      <td>-0.146651</td>\n",
       "      <td>0.651597</td>\n",
       "      <td>0.721892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064762</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.738236</td>\n",
       "      <td>-0.034588</td>\n",
       "      <td>0.414326</td>\n",
       "      <td>0.705330</td>\n",
       "      <td>0.079874</td>\n",
       "      <td>0.260653</td>\n",
       "      <td>-0.002987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the minds of the surrounding families, that...</td>\n",
       "      <td>0.076889</td>\n",
       "      <td>0.027343</td>\n",
       "      <td>0.093580</td>\n",
       "      <td>0.140345</td>\n",
       "      <td>0.367784</td>\n",
       "      <td>-0.355897</td>\n",
       "      <td>-0.146437</td>\n",
       "      <td>0.422682</td>\n",
       "      <td>1.344679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.454421</td>\n",
       "      <td>-0.589578</td>\n",
       "      <td>-0.519553</td>\n",
       "      <td>1.044313</td>\n",
       "      <td>0.315445</td>\n",
       "      <td>0.209593</td>\n",
       "      <td>-0.140470</td>\n",
       "      <td>0.346846</td>\n",
       "      <td>-0.364940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not. “But it is,” returned she; “for Mrs. Long...</td>\n",
       "      <td>0.339588</td>\n",
       "      <td>-0.940871</td>\n",
       "      <td>0.216766</td>\n",
       "      <td>0.602502</td>\n",
       "      <td>-0.659164</td>\n",
       "      <td>-0.278765</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>0.338654</td>\n",
       "      <td>0.699363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068579</td>\n",
       "      <td>-0.342067</td>\n",
       "      <td>-0.601994</td>\n",
       "      <td>0.441354</td>\n",
       "      <td>0.222831</td>\n",
       "      <td>-0.364223</td>\n",
       "      <td>0.014880</td>\n",
       "      <td>0.178083</td>\n",
       "      <td>-0.312789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hearing it.” [Illustration: “He came down to s...</td>\n",
       "      <td>0.149145</td>\n",
       "      <td>0.064928</td>\n",
       "      <td>0.101019</td>\n",
       "      <td>-0.188449</td>\n",
       "      <td>-0.492822</td>\n",
       "      <td>-0.129286</td>\n",
       "      <td>0.132692</td>\n",
       "      <td>0.402831</td>\n",
       "      <td>0.482742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533847</td>\n",
       "      <td>-0.665060</td>\n",
       "      <td>-0.823555</td>\n",
       "      <td>0.521699</td>\n",
       "      <td>0.257297</td>\n",
       "      <td>0.023976</td>\n",
       "      <td>-0.694296</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>-0.178064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in a chaise and four to see the place, and was...</td>\n",
       "      <td>0.146287</td>\n",
       "      <td>-0.309895</td>\n",
       "      <td>0.128825</td>\n",
       "      <td>-0.141531</td>\n",
       "      <td>-0.506306</td>\n",
       "      <td>0.131742</td>\n",
       "      <td>-0.344784</td>\n",
       "      <td>1.192157</td>\n",
       "      <td>0.450028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425849</td>\n",
       "      <td>-0.648054</td>\n",
       "      <td>-0.203158</td>\n",
       "      <td>0.706744</td>\n",
       "      <td>0.299172</td>\n",
       "      <td>0.223305</td>\n",
       "      <td>-0.565342</td>\n",
       "      <td>-0.069827</td>\n",
       "      <td>0.115047</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>respect which almost overcame her affection, s...</td>\n",
       "      <td>-0.010220</td>\n",
       "      <td>-0.190580</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>-0.037104</td>\n",
       "      <td>-1.292766</td>\n",
       "      <td>-0.183501</td>\n",
       "      <td>-0.409878</td>\n",
       "      <td>0.427105</td>\n",
       "      <td>-0.085560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191353</td>\n",
       "      <td>-0.386279</td>\n",
       "      <td>-0.792590</td>\n",
       "      <td>0.331757</td>\n",
       "      <td>-0.061435</td>\n",
       "      <td>-1.001084</td>\n",
       "      <td>-0.010191</td>\n",
       "      <td>0.232712</td>\n",
       "      <td>0.321188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>a sister more than ten years younger than hims...</td>\n",
       "      <td>0.043020</td>\n",
       "      <td>-0.176304</td>\n",
       "      <td>0.330010</td>\n",
       "      <td>0.180056</td>\n",
       "      <td>-0.655494</td>\n",
       "      <td>-0.232812</td>\n",
       "      <td>0.092122</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>-0.082347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120363</td>\n",
       "      <td>-0.810071</td>\n",
       "      <td>-0.149972</td>\n",
       "      <td>0.544790</td>\n",
       "      <td>0.078733</td>\n",
       "      <td>-0.531377</td>\n",
       "      <td>-0.502457</td>\n",
       "      <td>1.025330</td>\n",
       "      <td>-0.082401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>especially of Elizabeth, that for some time al...</td>\n",
       "      <td>-0.211630</td>\n",
       "      <td>-0.400246</td>\n",
       "      <td>0.100616</td>\n",
       "      <td>0.886872</td>\n",
       "      <td>-0.676006</td>\n",
       "      <td>-0.668235</td>\n",
       "      <td>-0.701038</td>\n",
       "      <td>-0.522564</td>\n",
       "      <td>0.110424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466563</td>\n",
       "      <td>-0.606127</td>\n",
       "      <td>-0.594379</td>\n",
       "      <td>0.529732</td>\n",
       "      <td>0.044274</td>\n",
       "      <td>-0.984253</td>\n",
       "      <td>-0.426623</td>\n",
       "      <td>0.508764</td>\n",
       "      <td>0.275507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>affection for him, or her curiosity to see how...</td>\n",
       "      <td>0.277620</td>\n",
       "      <td>-0.042760</td>\n",
       "      <td>0.356489</td>\n",
       "      <td>0.322990</td>\n",
       "      <td>-0.128166</td>\n",
       "      <td>0.036034</td>\n",
       "      <td>-0.428013</td>\n",
       "      <td>0.764357</td>\n",
       "      <td>0.315446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628144</td>\n",
       "      <td>-0.216785</td>\n",
       "      <td>-1.008442</td>\n",
       "      <td>0.543078</td>\n",
       "      <td>-0.381914</td>\n",
       "      <td>-0.627292</td>\n",
       "      <td>-0.306053</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>-0.119869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>the city. With the Gardiners they were always ...</td>\n",
       "      <td>0.035126</td>\n",
       "      <td>0.217893</td>\n",
       "      <td>0.117233</td>\n",
       "      <td>0.318431</td>\n",
       "      <td>-0.855791</td>\n",
       "      <td>-0.501050</td>\n",
       "      <td>0.167391</td>\n",
       "      <td>0.069115</td>\n",
       "      <td>1.005235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.568319</td>\n",
       "      <td>-0.768572</td>\n",
       "      <td>0.198092</td>\n",
       "      <td>0.863801</td>\n",
       "      <td>-0.043373</td>\n",
       "      <td>0.231857</td>\n",
       "      <td>-0.639644</td>\n",
       "      <td>0.611829</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2448 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Embedding_0  \\\n",
       "0     Chapter I.] It is a truth universally acknowle...     0.284331   \n",
       "1     in the minds of the surrounding families, that...     0.076889   \n",
       "2     not. “But it is,” returned she; “for Mrs. Long...     0.339588   \n",
       "3     hearing it.” [Illustration: “He came down to s...     0.149145   \n",
       "4     in a chaise and four to see the place, and was...     0.146287   \n",
       "...                                                 ...          ...   \n",
       "2443  respect which almost overcame her affection, s...    -0.010220   \n",
       "2444  a sister more than ten years younger than hims...     0.043020   \n",
       "2445  especially of Elizabeth, that for some time al...    -0.211630   \n",
       "2446  affection for him, or her curiosity to see how...     0.277620   \n",
       "2447  the city. With the Gardiners they were always ...     0.035126   \n",
       "\n",
       "      Embedding_1  Embedding_2  Embedding_3  Embedding_4  Embedding_5  \\\n",
       "0        0.198523     0.321001    -0.224241    -0.742237    -0.527830   \n",
       "1        0.027343     0.093580     0.140345     0.367784    -0.355897   \n",
       "2       -0.940871     0.216766     0.602502    -0.659164    -0.278765   \n",
       "3        0.064928     0.101019    -0.188449    -0.492822    -0.129286   \n",
       "4       -0.309895     0.128825    -0.141531    -0.506306     0.131742   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2443    -0.190580     0.207971    -0.037104    -1.292766    -0.183501   \n",
       "2444    -0.176304     0.330010     0.180056    -0.655494    -0.232812   \n",
       "2445    -0.400246     0.100616     0.886872    -0.676006    -0.668235   \n",
       "2446    -0.042760     0.356489     0.322990    -0.128166     0.036034   \n",
       "2447     0.217893     0.117233     0.318431    -0.855791    -0.501050   \n",
       "\n",
       "      Embedding_6  Embedding_7  Embedding_8  ...  Embedding_1015  \\\n",
       "0       -0.146651     0.651597     0.721892  ...        0.064762   \n",
       "1       -0.146437     0.422682     1.344679  ...       -0.454421   \n",
       "2       -0.031425     0.338654     0.699363  ...        0.068579   \n",
       "3        0.132692     0.402831     0.482742  ...       -0.533847   \n",
       "4       -0.344784     1.192157     0.450028  ...       -0.425849   \n",
       "...           ...          ...          ...  ...             ...   \n",
       "2443    -0.409878     0.427105    -0.085560  ...       -0.191353   \n",
       "2444     0.092122     0.034753    -0.082347  ...        0.120363   \n",
       "2445    -0.701038    -0.522564     0.110424  ...       -0.466563   \n",
       "2446    -0.428013     0.764357     0.315446  ...       -0.628144   \n",
       "2447     0.167391     0.069115     1.005235  ...       -0.568319   \n",
       "\n",
       "      Embedding_1016  Embedding_1017  Embedding_1018  Embedding_1019  \\\n",
       "0           0.014574       -0.738236       -0.034588        0.414326   \n",
       "1          -0.589578       -0.519553        1.044313        0.315445   \n",
       "2          -0.342067       -0.601994        0.441354        0.222831   \n",
       "3          -0.665060       -0.823555        0.521699        0.257297   \n",
       "4          -0.648054       -0.203158        0.706744        0.299172   \n",
       "...              ...             ...             ...             ...   \n",
       "2443       -0.386279       -0.792590        0.331757       -0.061435   \n",
       "2444       -0.810071       -0.149972        0.544790        0.078733   \n",
       "2445       -0.606127       -0.594379        0.529732        0.044274   \n",
       "2446       -0.216785       -1.008442        0.543078       -0.381914   \n",
       "2447       -0.768572        0.198092        0.863801       -0.043373   \n",
       "\n",
       "      Embedding_1020  Embedding_1021  Embedding_1022  Embedding_1023  Labels  \n",
       "0           0.705330        0.079874        0.260653       -0.002987       0  \n",
       "1           0.209593       -0.140470        0.346846       -0.364940       0  \n",
       "2          -0.364223        0.014880        0.178083       -0.312789       0  \n",
       "3           0.023976       -0.694296        0.005413       -0.178064       0  \n",
       "4           0.223305       -0.565342       -0.069827        0.115047       0  \n",
       "...              ...             ...             ...             ...     ...  \n",
       "2443       -1.001084       -0.010191        0.232712        0.321188       0  \n",
       "2444       -0.531377       -0.502457        1.025330       -0.082401       0  \n",
       "2445       -0.984253       -0.426623        0.508764        0.275507       0  \n",
       "2446       -0.627292       -0.306053        0.513492       -0.119869       0  \n",
       "2447        0.231857       -0.639644        0.611829       -0.031657       0  \n",
       "\n",
       "[2448 rows x 1026 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "text_data = read_chunks('../Texts/austen.txt')\n",
    "embeddings_data = np.load('../Texts/austen.npy') \n",
    "labels_data = y[:lengths[0]].ravel()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Text': text_data,\n",
    "})\n",
    "\n",
    "# Create a DataFrame for the embeddings\n",
    "embedding_columns = [f'Embedding_{i}' for i in range(embeddings_data.shape[1])]\n",
    "df_embeddings = pd.DataFrame(embeddings_data, columns=embedding_columns)\n",
    "df_labels = pd.DataFrame(labels_data, columns=['Labels'])\n",
    "\n",
    "# Concatenate the text/label DataFrame with the embeddings DataFrame\n",
    "df = pd.concat([df, df_embeddings, df_labels], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN / Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 1, 1,  ..., 4, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.ravel(), test_size=0.2, stratify=y.ravel())\n",
    "\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "dataset_train = TextDataset(torch.tensor(X_train), y_train)\n",
    "dataset_test = TextDataset(torch.tensor(X_test), y_test)\n",
    "\n",
    "# splitting the data into batches\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "dl_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dl_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# defining the model, optimizer and loss function\n",
    "model = NeuralNet()\n",
    "\n",
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(dataloader, model, loss_fn, optimizer, num_epochs=100):\n",
    "    size = len(dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # setting the model to train mode\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()           # resets the gradients\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * batch_size + len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode \n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():           # gradient computation excluded, unecessary\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(0) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.904862  [ 1024/13335]\n",
      "loss: 0.908846  [ 7424/13335]\n",
      "loss: 0.904832  [13824/13335]\n",
      "loss: 1.899796  [20224/13335]\n",
      "loss: 0.904832  [26624/13335]\n",
      "loss: 0.904840  [33024/13335]\n",
      "loss: 0.904832  [39424/13335]\n",
      "loss: 0.904832  [45824/13335]\n",
      "loss: 1.904827  [52224/13335]\n",
      "loss: 0.904889  [58624/13335]\n",
      "loss: 0.905327  [65024/13335]\n",
      "loss: 0.904832  [71424/13335]\n",
      "loss: 0.904832  [77824/13335]\n",
      "loss: 0.904832  [84224/13335]\n",
      "loss: 0.904833  [90624/13335]\n",
      "loss: 0.904832  [97024/13335]\n",
      "loss: 0.904900  [103424/13335]\n",
      "loss: 0.904833  [109824/13335]\n",
      "loss: 0.904859  [116224/13335]\n",
      "loss: 1.904756  [122624/13335]\n",
      "loss: 0.904832  [129024/13335]\n",
      "loss: 0.904832  [135424/13335]\n",
      "loss: 0.904832  [141824/13335]\n",
      "loss: 0.904832  [148224/13335]\n",
      "loss: 0.904832  [154624/13335]\n",
      "loss: 1.904281  [161024/13335]\n",
      "loss: 0.904832  [167424/13335]\n",
      "loss: 0.904834  [173824/13335]\n",
      "loss: 0.904832  [180224/13335]\n",
      "loss: 0.904833  [186624/13335]\n",
      "loss: 0.904832  [193024/13335]\n",
      "loss: 0.904862  [199424/13335]\n",
      "loss: 0.904832  [205824/13335]\n",
      "loss: 0.904874  [212224/13335]\n",
      "loss: 0.904899  [218624/13335]\n",
      "loss: 0.904832  [225024/13335]\n",
      "loss: 0.904832  [231424/13335]\n",
      "loss: 1.899967  [237824/13335]\n",
      "loss: 0.904832  [244224/13335]\n",
      "loss: 0.904832  [250624/13335]\n",
      "loss: 0.904832  [257024/13335]\n",
      "loss: 0.904836  [263424/13335]\n",
      "loss: 0.904832  [269824/13335]\n",
      "loss: 0.904832  [276224/13335]\n",
      "loss: 1.609932  [282624/13335]\n",
      "loss: 1.903386  [289024/13335]\n",
      "loss: 0.904832  [295424/13335]\n",
      "loss: 0.904832  [301824/13335]\n",
      "loss: 0.904832  [308224/13335]\n",
      "loss: 0.904832  [314624/13335]\n",
      "loss: 0.919337  [321024/13335]\n",
      "loss: 0.904833  [327424/13335]\n",
      "loss: 0.904832  [333824/13335]\n",
      "loss: 0.904957  [340224/13335]\n",
      "loss: 0.904832  [346624/13335]\n",
      "loss: 1.904533  [353024/13335]\n",
      "loss: 1.904186  [359424/13335]\n",
      "loss: 0.905262  [365824/13335]\n",
      "loss: 0.904832  [372224/13335]\n",
      "loss: 0.904832  [378624/13335]\n",
      "loss: 0.904833  [385024/13335]\n",
      "loss: 0.904832  [391424/13335]\n",
      "loss: 0.904832  [397824/13335]\n",
      "loss: 1.104599  [404224/13335]\n",
      "loss: 1.650442  [410624/13335]\n",
      "loss: 0.904837  [417024/13335]\n",
      "loss: 1.899553  [423424/13335]\n",
      "loss: 0.904833  [429824/13335]\n",
      "loss: 0.904832  [436224/13335]\n",
      "loss: 0.904832  [442624/13335]\n",
      "loss: 0.904832  [449024/13335]\n",
      "loss: 0.920172  [455424/13335]\n",
      "loss: 0.904835  [461824/13335]\n",
      "loss: 0.905396  [468224/13335]\n",
      "loss: 0.904832  [474624/13335]\n",
      "loss: 1.841096  [481024/13335]\n",
      "loss: 0.904832  [487424/13335]\n",
      "loss: 0.904832  [493824/13335]\n",
      "loss: 0.904832  [500224/13335]\n",
      "loss: 0.904832  [506624/13335]\n",
      "loss: 0.904836  [513024/13335]\n",
      "loss: 1.451736  [519424/13335]\n",
      "loss: 0.904832  [525824/13335]\n",
      "loss: 0.904832  [532224/13335]\n",
      "loss: 0.904832  [538624/13335]\n",
      "loss: 0.904833  [545024/13335]\n",
      "loss: 0.904832  [551424/13335]\n",
      "loss: 0.986283  [557824/13335]\n",
      "loss: 1.904820  [564224/13335]\n",
      "loss: 0.904832  [570624/13335]\n",
      "loss: 0.904832  [577024/13335]\n",
      "loss: 0.904832  [583424/13335]\n",
      "loss: 0.907845  [589824/13335]\n",
      "loss: 0.904832  [596224/13335]\n",
      "loss: 0.904832  [602624/13335]\n",
      "loss: 0.904832  [609024/13335]\n",
      "loss: 0.904832  [615424/13335]\n",
      "loss: 0.904832  [621824/13335]\n",
      "loss: 1.904829  [628224/13335]\n",
      "loss: 0.904832  [634624/13335]\n",
      "loss: 1.904832  [641024/13335]\n",
      "loss: 0.904832  [647424/13335]\n",
      "loss: 0.904833  [653824/13335]\n",
      "loss: 0.908759  [660224/13335]\n",
      "loss: 1.884667  [666624/13335]\n",
      "loss: 0.904832  [673024/13335]\n",
      "loss: 1.904454  [679424/13335]\n",
      "loss: 0.904835  [685824/13335]\n",
      "loss: 0.904832  [692224/13335]\n",
      "loss: 0.904832  [698624/13335]\n",
      "loss: 0.904836  [705024/13335]\n",
      "loss: 1.053118  [711424/13335]\n",
      "loss: 1.899103  [717824/13335]\n",
      "loss: 0.904832  [724224/13335]\n",
      "loss: 0.904832  [730624/13335]\n",
      "loss: 0.904832  [737024/13335]\n",
      "loss: 0.904832  [743424/13335]\n",
      "loss: 0.904832  [749824/13335]\n",
      "loss: 0.904833  [756224/13335]\n",
      "loss: 0.904832  [762624/13335]\n",
      "loss: 0.977957  [769024/13335]\n",
      "loss: 0.904832  [775424/13335]\n",
      "loss: 1.902878  [781824/13335]\n",
      "loss: 0.906056  [788224/13335]\n",
      "loss: 0.904832  [794624/13335]\n",
      "loss: 1.904832  [801024/13335]\n",
      "loss: 0.904832  [807424/13335]\n",
      "loss: 0.904832  [813824/13335]\n",
      "loss: 0.904832  [820224/13335]\n",
      "loss: 0.904832  [826624/13335]\n",
      "loss: 1.378879  [833024/13335]\n",
      "loss: 0.904832  [839424/13335]\n",
      "loss: 0.904833  [845824/13335]\n",
      "loss: 0.905153  [852224/13335]\n"
     ]
    }
   ],
   "source": [
    "training_model(dataset_train, model, loss_fn, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.057047 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(dataset_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
