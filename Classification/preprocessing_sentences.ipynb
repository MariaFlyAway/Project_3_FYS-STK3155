{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and embedding\n",
    "This file includes code which classifies text chunks as (Austen, Kafka, Sturluson, God and Dostoyevsky).\n",
    "The training data is text chunks from their respective works 'Pride and predjudice', 'King James Bible', 'The trial', 'Heimskringla' and 'Crime and punishment'. We obtain the texts from the Gutenberg Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from nltk import tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\fysstk-textembedding\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\maria\\anaconda3\\envs\\fysstk-textembedding\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\maria\\.cache\\huggingface\\hub\\models--mixedbread-ai--mxbai-embed-large-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Importing the transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\" \n",
    "\"\"\"\n",
    "Change the default model here. To save the model locally, replace the path with your path and run this module.\n",
    "At the end of your path, add the name of the folder you want create for the model, eg. \".../local_model\"\n",
    "When you have downloaded the model, uncomment model_name = path to use the stored model.\n",
    "\"\"\"\n",
    "# path = \"C:/Users/jonas/OneDrive/Dokumenter/Python Scripts/embed/local_model_sentence_transformers\" # (example path / for my convenience)\n",
    "# model_name = path # Uncomment this line once you have downloaded the model.\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocessing(filepath, text, end=False):\n",
    "    if 'austen' in filepath:\n",
    "        start = text.find(\"Chapter I.]\")\n",
    "\n",
    "    elif 'dostoyevsky' in filepath:\n",
    "        start = text.find(\"CHAPTER I\")\n",
    "\n",
    "    elif 'god' in filepath:\n",
    "        start = text.find(\"1:1\")\n",
    "        end = text.find(\"in the sight of all Israel.\") # Only old testament\n",
    "\n",
    "    elif 'kafka' in filepath:\n",
    "        start = text.find(\"Chapter One\")\n",
    "\n",
    "    elif 'shelley' in filepath:\n",
    "        start = text.find(\"_To\")\n",
    "\n",
    "    elif 'tolstoy' in filepath:\n",
    "        start = text.find(\"Chapter 1\")\n",
    "\n",
    "    elif 'sturluson' in filepath:\n",
    "        start = text.find(\"PREFACE OF SNORRE STURLASON.\")\n",
    "        end = text.find(\"SAGA OF HARALD HARDRADE.\") # Only Heimskringla\n",
    "        \n",
    "    elif 'cervantes' in filepath:\n",
    "        start = text.find(\"Idle reader:\")\n",
    "\n",
    "    elif 'brother_karamazov' in filepath:\n",
    "        start = text.find(\"Fyodor Pavlovitch Karamazov\")\n",
    "\n",
    "    elif 'sense_and_sensibility' in filepath:\n",
    "        start = text.find(\"CHAPTER I.\")\n",
    "\n",
    "    elif 'wells' in filepath:\n",
    "        start = text.find(\"Introduction\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"This book is not in our library!\")\n",
    "    \n",
    "    if not end:\n",
    "        end = text.find(\"*** END\")\n",
    "    \n",
    "    return text[start:end].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(filepath, Feedback=True):\n",
    "    with open(filepath, encoding='utf-8') as infile:\n",
    "        words = preprocessing(filepath, infile.read())\n",
    "        words = ' '.join(words)\n",
    "\n",
    "        sentences = tokenize.sent_tokenize(words)\n",
    "\n",
    "    if Feedback:\n",
    "        print(f\"Length: {len(sentences):,} sentences.\")\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embeddings.append(model.encode(chunk))\n",
    "    return np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Texts\\austen\n",
      "Length: 4,657 sentences.\n",
      "..\\Texts\\dostoyevsky\n",
      "Length: 11,906 sentences.\n",
      "..\\Texts\\god\n",
      "Length: 4,962 sentences.\n",
      "..\\Texts\\cervantes\n",
      "Length: 5,883 sentences.\n",
      "..\\Texts\\sturluson\n",
      "Length: 8,781 sentences.\n"
     ]
    }
   ],
   "source": [
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "filenames = ['austen', 'dostoyevsky', 'god', 'cervantes', 'sturluson']\n",
    "filepaths = [os.path.join(folder, subfolder, filename) for filename in filenames]\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "# Uncomment to embed\n",
    "for filepath in filepaths:\n",
    "    print(filepath)\n",
    "    sentences = split_sentences(filepath+'.txt') # remove max_len to run on entire text\n",
    "    all_sentences.append(sentences)\n",
    "    # embeddings = embed(sentences)\n",
    "    # np.save(filepath+'_sentences', embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2500\n",
    "sent_reduced = []\n",
    "embeddings_red = []\n",
    "length_threshold = 100\n",
    "filepath = '../Texts/'\n",
    "files = ['austen_sentences.npy', 'dostoyevsky_sentences.npy', 'god_sentences.npy', 'cervantes_sentences.npy', 'sturluson_sentences.npy']\n",
    "\n",
    "for idx, sent in enumerate(all_sentences):\n",
    "    sentences_array = np.array(sent)\n",
    "    embedding = np.load(filepath+files[idx])\n",
    "\n",
    "    # Create a vectorized function to check the length of each sentence\n",
    "    length_check = np.vectorize(lambda s: len(s) >= length_threshold)\n",
    "\n",
    "    # creating a mask and applying it to the sentences and embeddings\n",
    "    mask = length_check(sentences_array)\n",
    "    filtered_sentences = sentences_array[mask]\n",
    "    embedding = embedding[mask]\n",
    "\n",
    "    # picking out 2500 random sentences\n",
    "    indices = np.random.choice(np.arange(0, len(filtered_sentences)), size=n, replace=False)\n",
    "\n",
    "    filtered_sentences = filtered_sentences[indices]\n",
    "    embedding = embedding[indices]\n",
    "\n",
    "    sent_reduced.append(filtered_sentences)\n",
    "    embeddings_red.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(embeddings_red)\n",
    "y = np.repeat(np.arange(5), 2500) # improved target vector\n",
    "\n",
    "np.save('text_data_sentences', X)\n",
    "np.save('labels_sentences', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataframe with embeddings and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sent_reduced).ravel()\n",
    "embedding = np.array(embeddings_red).reshape(-1, 1024)\n",
    "\n",
    "df = pd.DataFrame(embedding, columns=[f\"component {i}\" for i in range(embedding.shape[1])])\n",
    "df.insert(0, \"sentence\", sentences)\n",
    "\n",
    "df.to_csv('../Classification/sentences_embeddings', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk-textembedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
