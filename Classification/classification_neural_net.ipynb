{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "This file includes code which classifies text chunks as (Austen, Shelly, Kafka, Tolstoy or Dostoyevsky).\n",
    "The training data is text chunks from their respective works _Pride and predjudice_, _Frankenstein_, _The trial_, _Anna Karenina_ and _Crime and punishment_. We obtain the texts from the Gutenberg Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from textdataset import TextDataset\n",
    "from neural_net import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `mixedbread-ai`-embedding model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\" \n",
    "\"\"\"\n",
    "Change the default model here. To save the model locally, replace the path with your path and run this module.\n",
    "At the end of your path, add the name of the folder you want create for the model, eg. \".../local_model\"\n",
    "When you have downloaded the model, uncomment model_name = path to use the stored model.\n",
    "\"\"\"\n",
    "# path = \"C:/Users/jonas/OneDrive/Dokumenter/Python Scripts/embed/local_model_sentence_transformers\" # (example path / for my convenience)\n",
    "# model_name = path # Uncomment this line once you have downloaded the model.\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocessing(filepath, text):\n",
    "    if 'austen' in filepath:\n",
    "        start = text.find(\"Chapter I.]\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'dostoyevsky' in filepath:\n",
    "        start = text.find(\"CHAPTER I\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'kafka' in filepath:\n",
    "        start = text.find(\"Chapter One\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'shelley' in filepath:\n",
    "        start = text.find(\"_To\")\n",
    "        # potential other preprocessing\n",
    "\n",
    "    elif 'tolstoy' in filepath:\n",
    "        start = text.find(\"Chapter 1\")\n",
    "        # potential other preprocessing\n",
    "    else:\n",
    "        raise Exception(\"This book is not in our library!\")\n",
    "    \n",
    "    end = text.find(\"*** END\")\n",
    "    return text[start:end].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(filepath, chunksize=50, max_len=0, Feedback=True):\n",
    "    \"\"\"\n",
    "    Reads text into a list of strings with the specified number of words (discards final chunk to ensure similar length).\n",
    "    \"\"\"\n",
    "    with open(filepath, encoding='utf-8') as infile:\n",
    "        words = preprocessing( filepath, infile.read() ) # list of words, preprocessed\n",
    "        if max_len:\n",
    "            assert max_len > chunksize\n",
    "            words = words[:max_len]\n",
    "\n",
    "        length_words = len(words)\n",
    "        n_chunks = int(length_words / chunksize)\n",
    "        chunks = np.empty(n_chunks, dtype=object)\n",
    "        for i, start in enumerate(range(0, length_words, chunksize)):\n",
    "            if start+chunksize < length_words: # we discard the final chunk if it is shorter than 50 words\n",
    "                chunks[i] = ' '.join(words[start:start+chunksize])\n",
    "\n",
    "        if Feedback:\n",
    "            print(f\"Length: {length_words:,} words, on {n_chunks:,} chunks of length {chunksize}.\")\n",
    "        \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embeddings.append( model.encode(chunk) )\n",
    "    return np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change max_len and run on clusters - this might take days on the full dataset (but then will never have to be done again:)\n",
    "folder = \"..\"\n",
    "subfolder = \"Texts\"\n",
    "filenames = ['austen', 'dostoyevsky', 'kafka', 'shelley', 'tolstoy']\n",
    "filepaths = [os.path.join(folder, subfolder, filename) for filename in filenames]\n",
    "\n",
    "# for filepath in filepaths:\n",
    "#     print(filepath)\n",
    "#     chunks = read_chunks(filepath+'.txt') # remove max_len to run on entire text\n",
    "#     embeddings = embed(chunks)\n",
    "#     np.save(filepath, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [4]\n",
      " [4]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "# Class encoding (0 through 4, alphabetically)\n",
    "data = [np.load(filepath+'.npy') for filepath in filepaths]\n",
    "lengths = [len(author) for author in data]\n",
    "\n",
    "X = np.vstack( data )\n",
    "y = np.vstack( [[[i]]*length for i, length in enumerate(lengths)] ) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN / Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.ravel(), test_size=0.2, stratify=y.ravel(), random_state=3)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "dataset_train = TextDataset(torch.tensor(X_train), y_train)\n",
    "dataset_test = TextDataset(torch.tensor(X_test), y_test)\n",
    "\n",
    "# splitting the data into batches\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "dl_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dl_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# defining the model, optimizer and loss function\n",
    "model = NeuralNet()\n",
    "\n",
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(dataloader, model, loss_fn, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        # setting the model to train mode\n",
    "        model.train()\n",
    "        \n",
    "        for (X, y) in dataloader:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()           # resets the gradients\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            loss = loss.item()\n",
    "            print(f\"Epoch {epoch+1:>4f}      loss: {loss:>7f}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode \n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():           # gradient computation excluded, unecessary\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(0) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\fysstk3155\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.000000      loss: 0.904832\n"
     ]
    }
   ],
   "source": [
    "training_model(dataset_train, model, loss_fn, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 1.048907 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(dataset_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Assuming you have a test dataloader `test_dataloader`, a trained model `model`, and a loss function `loss_fn`\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 19\u001b[0m, in \u001b[0;36mtest_loop\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m     18\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 19\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Store predictions and true labels\u001b[39;00m\n\u001b[0;32m     22\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode \n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():           # gradient computation excluded, unnecessary\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # Compute and print confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a test dataloader `test_dataloader`, a trained model `model`, and a loss function `loss_fn`\n",
    "test_loop(dataset_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn wrapping using `skorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    NeuralNet,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "net.fit(X, y)\n",
    "y_proba = net.predict_proba(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
